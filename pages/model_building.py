# æ¨¡å‹æ„å»º

import os
import sys
import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
import folium
import altair as alt
import streamlit as st
import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error as MSE
import streamlit.components.v1 as components

def app():
    st.header('åœè½¦å æœ‰ç‡é¢„æµ‹')
    st.session_state.info_st.success("åœè½¦å æœ‰ç‡é¢„æµ‹æ¨¡å‹æ„å»ºğŸ‘‰")

    # å®šä¹‰å…¨å±€å˜é‡
    data = pd.read_csv(os.path.join(st.session_state.data_output, 'timeSeriesFeatures.csv'), index_col=0)
    locations = pd.read_csv(os.path.join(st.session_state.data_output, 'locations_processed.csv'))
    col = st.selectbox(
        'è¯·é€‰æ‹©æ¨¡å‹è®­ç»ƒçš„åœè½¦åœº:',
        data.columns
        )

    st.write("---")
    st.subheader("æ•°æ®é¢„å¤„ç†")
    train_dataset, train_labels, test_dataset, test_labels, train_batch_dataset, test_batch_dataset = preprocess(data, locations, col)

    st.write("---")
    st.subheader("æ¨¡å‹è®­ç»ƒ")
    training(col, train_dataset, train_batch_dataset, test_batch_dataset, epochs=30)

    if not st.session_state.simplified_mode:
        st.write("---")
        st.subheader("æ¨¡å‹è¯„ä¼°")
        evaluate()

    st.write("---")
    st.subheader("æ¨¡å‹é¢„æµ‹")
    prediction(col, train_dataset, train_labels, test_dataset, test_labels)

    st.write('---')
    st.subheader('åŠ¨æ€é¢„æµ‹çƒ­åŠ›å›¾')
    labels_list, pred_list = plot_HeatMapWithTime(locations=locations)
    lon, lat = locations['longtitude'].mean(), locations['latitude'].mean()
    m = folium.plugins.DualMap(location=(lat, lon), zoom_start=14)
    folium.plugins.HeatMapWithTime(data=labels_list,auto_play=True, radius=60, display_index=False, name='åŸå§‹æ•°æ®').add_to(m.m1)
    folium.plugins.HeatMapWithTime(data=pred_list, auto_play=True, radius=60, display_index=False, name='é¢„æµ‹æ•°æ®').add_to(m.m2)
    folium.LayerControl(collapsed=False).add_to(m)
    m.save(os.path.join(st.session_state.data_output, 'map.html'))
    map_html = open(os.path.join(st.session_state.data_output, 'map.html'),"r",encoding='utf-8').read()
    components.html(html=map_html, height=500)

def preprocess(data, locations, col):
    train_ratio = 0.8
    SEQ_LEN = 18  # 8:00 - 16:30 çš„æ•°æ®é•¿åº¦
    batch_size = 32

    temp = st.info("åˆ’åˆ†ç‰¹å¾å’Œæ ‡ç­¾")
    X = data
    y = data.loc[:, data.columns == col]
    st.write('ç‰¹å¾ç»´åº¦(æ—¶é—´åºåˆ—é•¿åº¦, åœè½¦åœºæ•°æ®): ', X.shape)
    st.write('æ ‡ç­¾ç»´åº¦(æ—¶é—´åºåˆ—é•¿åº¦, 1): ', y.shape)
    Spatialweight = locations[col]
    st.write(col, 'åœè½¦åœºçš„ç©ºé—´æƒé‡ï¼š', pd.DataFrame(Spatialweight).T)
    X = X.mul(list(Spatialweight))
    temp.success("åˆ’åˆ†ç‰¹å¾å’Œæ ‡ç­¾")

    temp = st.info("åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†")
    st.write('è®­ç»ƒé›†å æ¯”: ', train_ratio)
    Xtrain, Xtest, Ytrain, Ytest = split_dataset(X, y, train_ratio=train_ratio)
    altdata = data.reset_index()
    altdata['index'] = altdata.index
    altdata['train_valid'] = ["Train" if x <=len(Ytrain) else "Vaild" for x in altdata.index]
    line = alt.Chart(altdata).mark_line().encode(
        x='index:Q',
        y=f'{col}:Q',
        color=alt.Color('train_valid:N', legend=None),
    )
    st.altair_chart(line, use_container_width=True)
    temp.success("åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†")

    temp = st.info("æ„é€ æ—¶é—´åºåˆ—æ•°æ®é›†å¹¶è¿›è¡Œæ‰¹å¤„ç†")
    st.write('LSTM æ»‘åŠ¨çª—å£é•¿åº¦: ', SEQ_LEN)
    st.write('æ‰¹å¤„ç†çš„batch_size: ', batch_size)
    train_dataset, train_labels = create_dataset(Xtrain, Ytrain, seq_len=SEQ_LEN)
    test_dataset, test_labels = create_dataset(Xtest, Ytest, seq_len=SEQ_LEN)
    st.write('æ—¶é—´åºåˆ—ç‰¹å¾ç»´åº¦(è®­ç»ƒé›†é•¿åº¦, æ»‘åŠ¨çª—å£é•¿åº¦, ç‰¹å¾ç»´åº¦): ', train_dataset.shape)
    st.write('æ—¶é—´åºåˆ—æ ‡ç­¾ç»´åº¦(è®­ç»ƒé›†é•¿åº¦, æ ‡ç­¾ç»´åº¦): ', train_labels.shape)
    train_batch_dataset = create_batch_dataset(train_dataset, train_labels, batch_size=batch_size)
    test_batch_dataset = create_batch_dataset(test_dataset, test_labels, train=False, batch_size=batch_size)
    st.write("æ„å»ºæ‰¹æ•°æ®çš„ç›®çš„æ˜¯åŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚")
    temp.success("æ„é€ æ—¶é—´åºåˆ—æ•°æ®é›†å¹¶è¿›è¡Œæ‰¹å¤„ç†")
    return train_dataset, train_labels, test_dataset, test_labels, train_batch_dataset, test_batch_dataset

def training(col, train_dataset, train_batch_dataset, test_batch_dataset, epochs=30):
    if os.path.exists(os.path.join('./data/output/models/', col)):
        temp = st.info('æ¨¡å‹æ­£åœ¨ä»äº‘ç«¯åŠ è½½...')
        model = tf.keras.models.load_model(os.path.join('./data/output/models/', col))
        temp.success('æ¨¡å‹åŠæƒé‡å·²æˆåŠŸåŠ è½½ï¼')
    else:
        temp = st.info('è®­ç»ƒ LSTM ç¥ç»ç½‘ç»œ...')
        model = keras.Sequential([
            keras.layers.LSTM(128, input_shape=train_dataset.shape[-2:], return_sequences=True),
            keras.layers.Dropout(0.5),
            keras.layers.LSTM(64),
            keras.layers.Dense(1)  # å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºä¸º1
        ])
        log_dir=f"./data/output/logs/fit/{col}/" + (datetime.datetime.now()+ datetime.timedelta(hours=8)).strftime("%Y%m%d-%H%M%S")
        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
        st.write('æ¨¡å‹ä¼˜åŒ–å‡½æ•°: adam')
        st.write('æ¨¡å‹æŸå¤±å‡½æ•°: mse')
        st.write('æ¨¡å‹è®­ç»ƒè½®æ¬¡: ', epochs)
        model.compile(optimizer='adam', loss="mse")
        history = model.fit(train_batch_dataset,
            epochs=epochs,
            validation_data=test_batch_dataset,
            callbacks=[tensorboard_callback],
            verbose=0)  # æ²‰é»˜è¾“å‡º
        model.save(os.path.join('./data/output/models/', col), save_format='h5')
        temp.success('æ¨¡å‹è®­ç»ƒå®Œæ¯•ï¼')


def evaluate():
    import ssl
    from pyngrok import ngrok
    import streamlit.components.v1 as components
    ssl._create_default_https_context = ssl._create_unverified_context
    os.system(f'ngrok authtoken {st.secrets["NGROK_TOKEN"]}')
    
    if len(ngrok.get_tunnels()) == 0:                                            
        # è‹¥æ²¡æœ‰ç½‘é¡µï¼Œåˆ™å¼€å¯ç«¯å£ç”Ÿæˆç½‘é¡µ
        if sys.platform.startswith('win'):
            os.system('start tensorboard --logdir ./data/output/logs/fit/ --port 6006')  # start å¼€å¯æ–°è¿›ç¨‹
        elif sys.platform.startswith('linux'):
            os.system('tensorboard --logdir ./data/output/logs/fit/ --port 6006 &')  # & å¼€å¯æ–°è¿›ç¨‹
        http_tunnel = ngrok.connect(addr='6006', proto='http', bind_tls=True)
    if len(ngrok.get_tunnels()) == 1:
        if st.button('é‡æ–°åŠ è½½ç½‘é¡µ'):
            for i in ngrok.get_tunnels():
                ngrok.disconnect(i.public_url)
            http_tunnel = ngrok.connect(addr='6006', proto='http', bind_tls=True)
        # è‹¥å·²æœ‰ç½‘é¡µï¼Œåˆ™ç›´æ¥è·å–ç½‘é¡µ
        st.write('è®¿é—®ç½‘é¡µ: ', ngrok.get_tunnels()[0].public_url)
        components.iframe(ngrok.get_tunnels()[0].public_url, height=600, scrolling=True)
    if len(ngrok.get_tunnels()) >= 2:
        # è‹¥ç½‘é¡µæ•°é‡å¤§äº2ï¼Œåˆ™æ¸…ç©ºç½‘é¡µ
        st.session_state.info_st.info('TensorBoardå¯åŠ¨å¤±è´¥,è¯·åˆ·æ–°é¡µé¢é‡è¯•!')
        for i in ngrok.get_tunnels():
            ngrok.disconnect(i.public_url)

def prediction(col, train_dataset, train_labels, test_dataset, test_labels):
    if not os.path.exists(os.path.join('./data/output/models/', col)):
        st.session_state.info_st.error("æœªå‘ç°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·é‡æ–°è®­ç»ƒæ¨¡å‹ï¼")
    else:
        model = tf.keras.models.load_model(os.path.join('./data/output/models/', col))
        col1, col2 = st.columns(2)
        with col1.expander("è®­ç»ƒé›†é¢„æµ‹", expanded=True):
            # è®­ç»ƒé›†çš„é¢„æµ‹
            train_pred = model.predict(train_dataset)
            plot_predict(train_labels,train_pred)
        with col2.expander("æµ‹è¯•é›†é¢„æµ‹", expanded=True):
            # æµ‹è¯•é›†çš„é¢„æµ‹
            test_pred = model.predict(test_dataset)
            plot_predict(test_labels,test_pred)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
def split_dataset(X, y, train_ratio=0.8):
    '''åŸºäºXå’Œy, åˆ‡åˆ†ä¸ºtrainå’Œtest
    Params:
        X : ç‰¹å¾æ•°æ®é›†
        y : æ ‡ç­¾æ•°æ®é›†
        train_ratio : è®­ç»ƒé›†å Xçš„æ¯”ä¾‹
    Returns:
        X_train, X_test, y_train, y_test
    '''
    X_len = len(X) # ç‰¹å¾æ•°æ®é›†Xçš„æ ·æœ¬æ•°é‡
    train_data_len = int(X_len * train_ratio) # è®­ç»ƒé›†çš„æ ·æœ¬æ•°é‡
    X_train = X[:train_data_len] # è®­ç»ƒé›†
    y_train = y[:train_data_len] # è®­ç»ƒæ ‡ç­¾é›†
    X_test = X[train_data_len:] # æµ‹è¯•é›†
    y_test = y[train_data_len:] # æµ‹è¯•é›†æ ‡ç­¾é›†
    # è¿”å›å€¼
    return X_train, X_test, y_train, y_test

# æ„é€ æ—¶é—´åºåˆ—æ•°æ®é›†
def create_dataset(X, y, seq_len=10):
    features = []
    targets = []
    for i in range(0, len(X) - seq_len, 1):
        data = X.iloc[i:i+seq_len] # åºåˆ—æ•°æ®ï¼ˆé•¿åº¦ä¸ºseq_lençš„å‘é‡ï¼‰
        label = y.iloc[i+seq_len] # æ ‡ç­¾æ•°æ®ï¼ˆé•¿åº¦ä¸º1ï¼‰
        # ä¿å­˜åˆ°featureså’Œlabels
        features.append(data)
        targets.append(label)
    return np.array(features), np.array(targets)

# æ„é€ æ‰¹è®­ç»ƒæ•°æ®ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒï¼ˆæ³¨æ„åŒºåˆ«è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰
def create_batch_dataset(X, y, train=True, buffer_size=100, batch_size=32):
    batch_data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y))) # æ•°æ®å°è£…ï¼Œtensorç±»å‹
    if train: # è®­ç»ƒé›†
        return batch_data.cache().shuffle(buffer_size).batch(batch_size)
    else: # æµ‹è¯•é›†
        return batch_data.batch(batch_size)

# ç»˜åˆ¶è¯¯å·®æ›²çº¿
def plot_predict(Ytest, Ypred):
    r2 = r2_score(Ytest, Ypred)
    rmse = np.sqrt(MSE(Ytest,Ypred))
    st.write('R2:', round(r2,3), ' RMSE:', round(rmse,3))
    fig = plt.figure(figsize=(8,4))
    plt.plot(range(len(Ytest)), Ytest, c='r', alpha=0.8, label='True')
    plt.plot(range(len(Ytest)), Ypred, c='b', label='Pred')
    plt.legend(fontsize=10)
    st.pyplot(fig)

# åˆ›å»ºåŠ¨æ€çƒ­åŠ›å›¾åºåˆ—
@st.cache
def plot_HeatMapWithTime(locations):
    parking_df = pd.read_pickle('./data/output/parking.pkl')
    SystemCodeNumber = locations['SystemCodeNumber'].unique()
    labels_list = []
    pred_list = []
    for time_id in range(len(parking_df.loc['test_labels'][0])):
        parking_labels_list = []
        parking_pred_list = []
        for parking_id in SystemCodeNumber:
            parking_labels_list.append([
                locations.loc[locations['SystemCodeNumber']==parking_id, 'latitude'].values[0],
                locations.loc[locations['SystemCodeNumber']==parking_id, 'longtitude'].values[0],
                parking_df.loc['test_labels', parking_id][time_id]
            ])
            parking_pred_list.append([
                locations.loc[locations['SystemCodeNumber']==parking_id, 'latitude'].values[0],
                locations.loc[locations['SystemCodeNumber']==parking_id, 'longtitude'].values[0],
                parking_df.loc['test_pred', parking_id][time_id][0]
            ])
        labels_list.append(parking_labels_list)
        pred_list.append(parking_pred_list)
    # é—´éš”3ä¸ªå–å€¼ï¼Œè¾¾åˆ°é—´éš”3å¸§æ’­æ”¾çš„æ•ˆæœ
    labels_list = labels_list[0:len(labels_list):3]
    pred_list = pred_list[0:len(pred_list):3]
    return labels_list, pred_list